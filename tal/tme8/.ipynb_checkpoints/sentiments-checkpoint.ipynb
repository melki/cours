{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import Counter,defaultdict\n",
    "import nltk.corpus.reader as pt\n",
    "import os\n",
    "\n",
    "import string\n",
    "import pdb\n",
    "import nltk\n",
    "import scipy\n",
    "import glob, os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readAFile(nf):\n",
    "    f = open(nf, 'rb')\n",
    "    l = []\n",
    "    txt = f.readlines()\n",
    "    for i in txt:\n",
    "        l.append(i.decode(\"utf-8\"))\n",
    "    f.close()\n",
    "    l = ' '.join(l)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    #txt = txt[txt.find(\"\\n\\n\"):] # elimination de l'entete (on ne conserve que les caractères après la première occurence du motif\n",
    "    txt = unicodedata.normalize(\"NFKD\",txt).encode(\"ascii\",\"ignore\") # elimination des caractères spéciaux, accents...\n",
    "\n",
    "    punc = string.punctuation    # recupération de la ponctuation\n",
    "    punc += u'\\n\\r\\t\\\\'          # ajouts de caractères à enlever\n",
    "    table =string.maketrans(punc, ' '*len(punc))  # table de conversion punc -> espace\n",
    "    txt = string.translate(txt,table).lower() # elimination des accents + minuscules\n",
    "    return txt\n",
    "    #return re.sub(\" +\",\" \", txt) # expression régulière pour transformer les espaces multiples en simples espaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#read positif \n",
    "os.chdir(\"/home/melki/Documents/cours/tal/tme8/pos\")\n",
    "pos = []\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    pos.append(readAFile(file))\n",
    "print len(pos)\n",
    "\n",
    "#read positif \n",
    "os.chdir(\"/home/melki/Documents/cours/tal/tme8/neg\")\n",
    "neg = []\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    neg.append(readAFile(file))\n",
    "print len(neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "X = pos + neg\n",
    "Y = [1 for i in range(len(pos))]+[-1 for i in range(len(neg))]\n",
    "print len(X)\n",
    "print len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "stopList = []\n",
    "\n",
    "# On instancie le tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "french_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "for a in range(len(X)):\n",
    "\n",
    "    # L'interface étant identique, le reste du code est le même\n",
    "    tokens = tokenizer.tokenize(X[a])\n",
    "\n",
    "\n",
    "    # chargement des stopwords français\n",
    "\n",
    "    # un petit filtre\n",
    "    tokens = [token for token in tokens if token.lower() not in french_stopwords]\n",
    "\n",
    "    stopList.append(' '.join(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "countVe = CountVectorizer(strip_accents='unicode',max_df=0.95, min_df=1, analyzer=\"word\",ngram_range=(1, 3) )\n",
    "\n",
    "tfve = TfidfVectorizer(strip_accents='unicode' ,max_df=0.9, min_df=1, analyzer=\"word\",ngram_range=(1, 3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = tfve.fit_transform(stopList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x1247054 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1916445 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as lin\n",
    "\n",
    "# données ultra basiques, à remplacer par vos corpus vectorisés\n",
    "X = idf\n",
    "# X = count\n",
    "y = np.array(Y)\n",
    "\n",
    "# SVM\n",
    "clf = svm.LinearSVC(C=1,max_iter=1000000)\n",
    "# apprentissage\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/melki/Documents/cours/tal/tme8\")\n",
    "testA = readAFile(\"testSentiment.txt\")\n",
    "\n",
    "test=[process(unicode(xi)) for xi in test]\n",
    "\n",
    "testList = []\n",
    "\n",
    "# On instancie le tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "french_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "for a in range(len(test)):\n",
    "\n",
    "    # L'interface étant identique, le reste du code est le même\n",
    "    tokens = tokenizer.tokenize(test[a])\n",
    "\n",
    "\n",
    "    # chargement des stopwords français\n",
    "\n",
    "    # un petit filtre\n",
    "    tokens = [token for token in tokens if token.lower() not in french_stopwords]\n",
    "\n",
    "    testList.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tfve.transform(testA)\n",
    "\n",
    "pp = clf.predict(a) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story of a man who has unnatural feelings for a pig  starts out with a opening scene that is a terrific example of absurd comedy  a formal orchestra audience is turned into an insane  violent mob by the crazy chantings of it s singers  unfortunately it stays absurd the whole time with no general narrative eventually making it just too off putting  even those from the era should be turned off  the cryptic dialogue would make shakespeare seem easy to a third grader  on a technical level it s better than you might think with some good cinematography by future great vilmos zsigmond  future stars sally kirkland and frederic forrest can be seen briefly \n"
     ]
    }
   ],
   "source": [
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open('stm.txt', 'w')\n",
    "for i in pp:\n",
    "    if i == -1:\n",
    "        f.write('C\\n')\n",
    "    else:\n",
    "        f.write('M\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
