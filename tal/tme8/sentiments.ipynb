{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import Counter,defaultdict\n",
    "import nltk.corpus.reader as pt\n",
    "import os\n",
    "\n",
    "import string\n",
    "import pdb\n",
    "import nltk\n",
    "import scipy\n",
    "import glob, os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readAFile(nf):\n",
    "    f = open(nf, 'rb')\n",
    "    l = []\n",
    "    txt = f.readlines()\n",
    "    for i in txt:\n",
    "        l.append(i.decode(\"utf-8\"))\n",
    "    f.close()\n",
    "    l = ' '.join(l)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    #txt = txt[txt.find(\"\\n\\n\"):] # elimination de l'entete (on ne conserve que les caractères après la première occurence du motif\n",
    "    txt = unicodedata.normalize(\"NFKD\",txt).encode(\"ascii\",\"ignore\") # elimination des caractères spéciaux, accents...\n",
    "\n",
    "    punc = string.punctuation    # recupération de la ponctuation\n",
    "    punc += u'\\n\\r\\t\\\\'          # ajouts de caractères à enlever\n",
    "    table =string.maketrans(punc, ' '*len(punc))  # table de conversion punc -> espace\n",
    "    txt = string.translate(txt,table).lower() # elimination des accents + minuscules\n",
    "    return txt\n",
    "    #return re.sub(\" +\",\" \", txt) # expression régulière pour transformer les espaces multiples en simples espaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#read positif \n",
    "os.chdir(\"/home/melki/Documents/cours/tal/tme8/pos\")\n",
    "pos = []\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    pos.append(readAFile(file))\n",
    "print len(pos)\n",
    "\n",
    "#read positif \n",
    "os.chdir(\"/home/melki/Documents/cours/tal/tme8/neg\")\n",
    "neg = []\n",
    "for file in glob.glob(\"*.txt\"):\n",
    "    neg.append(readAFile(file))\n",
    "print len(neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "X = pos + neg\n",
    "Y = [1 for i in range(len(pos))]+[-1 for i in range(len(neg))]\n",
    "print len(X)\n",
    "print len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "stopList = []\n",
    "\n",
    "# On instancie le tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "french_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "for a in range(len(X)):\n",
    "\n",
    "    # L'interface étant identique, le reste du code est le même\n",
    "    tokens = tokenizer.tokenize(X[a])\n",
    "\n",
    "\n",
    "    # chargement des stopwords français\n",
    "\n",
    "    # un petit filtre\n",
    "    tokens = [token for token in tokens if token.lower() not in french_stopwords]\n",
    "\n",
    "    stopList.append(' '.join(tokens))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "countVe = CountVectorizer(strip_accents='unicode',max_df=0.95, min_df=1, analyzer=\"word\",ngram_range=(1, 3) )\n",
    "\n",
    "tfve = TfidfVectorizer(strip_accents='unicode' ,max_df=0.9, min_df=1, analyzer=\"word\",ngram_range=(1, 3) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf = tfve.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x1529692 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 3039234 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as lin\n",
    "\n",
    "# données ultra basiques, à remplacer par vos corpus vectorisés\n",
    "X = idf\n",
    "# X = count\n",
    "y = np.array(Y)\n",
    "\n",
    "# SVM\n",
    "clf = svm.LinearSVC(C=1,max_iter=1000000)\n",
    "# apprentissage\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/melki/Documents/cours/tal/tme8\")\n",
    "testA = readAFile(\"testSentiment.txt\")\n",
    "testA = testA.split('\\n')[0:-1]\n",
    "# test=[process(unicode(xi)) for xi in test]\n",
    "\n",
    "# testList = []\n",
    "\n",
    "# # On instancie le tokenizer\n",
    "# tokenizer = WordPunctTokenizer()\n",
    "# french_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# for a in range(len(test)):\n",
    "\n",
    "#     # L'interface étant identique, le reste du code est le même\n",
    "#     tokens = tokenizer.tokenize(test[a])\n",
    "\n",
    "\n",
    "#     # chargement des stopwords français\n",
    "\n",
    "#     # un petit filtre\n",
    "#     tokens = [token for token in tokens if token.lower() not in french_stopwords]\n",
    "\n",
    "#     testList.append(' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = tfve.transform(testA)\n",
    "\n",
    "pp = clf.predict(a) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open('stm.txt', 'w')\n",
    "for i in pp:\n",
    "    if i == -1:\n",
    "        f.write('C\\n')\n",
    "    else:\n",
    "        f.write('M\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(testA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
