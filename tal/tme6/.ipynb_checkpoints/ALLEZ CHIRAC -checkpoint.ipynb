{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata\n",
    "import re\n",
    "from collections import Counter,defaultdict\n",
    "import nltk.corpus.reader as pt\n",
    "import os\n",
    "\n",
    "import string\n",
    "import pdb\n",
    "import nltk\n",
    "import scipy\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readAFile(nf):\n",
    "    f = open(nf, 'rb')\n",
    "    l = []\n",
    "    txt = f.readlines()\n",
    "    for i in txt:\n",
    "        l.append(i.decode(\"utf-8\"))\n",
    "    f.close()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process(txt):\n",
    "    #txt = txt[txt.find(\"\\n\\n\"):] # elimination de l'entete (on ne conserve que les caractères après la première occurence du motif\n",
    "    txt = unicodedata.normalize(\"NFKD\",txt).encode(\"ascii\",\"ignore\") # elimination des caractères spéciaux, accents...\n",
    "\n",
    "    punc = string.punctuation    # recupération de la ponctuation\n",
    "    punc += u'\\n\\r\\t\\\\'          # ajouts de caractères à enlever\n",
    "    table =string.maketrans(punc, ' '*len(punc))  # table de conversion punc -> espace\n",
    "    txt = string.translate(txt,table).lower() # elimination des accents + minuscules\n",
    "    return txt\n",
    "    #return re.sub(\" +\",\" \", txt) # expression régulière pour transformer les espaces multiples en simples espaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    }
   ],
   "source": [
    "t = readAFile('train.utf8')\n",
    "x=[]\n",
    "y=[]\n",
    "for txt in t:\n",
    "    lab = re.sub(r\"<[0-9]*:[0-9]*:(.)>.*\",\"\\\\1\",txt)\n",
    "    txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "    x.append(txt)\n",
    "    #print lab\n",
    "    if('C' in lab):\n",
    "        bin_lab=1\n",
    "    elif('M' in lab):\n",
    "        bin_lab=0\n",
    "    else:\n",
    "        bin_lab='err'\n",
    "    y.append(bin_lab)\n",
    "x=[process(xi) for xi in x]\n",
    "print x[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "stopList = []\n",
    "\n",
    "# On instancie le tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "\n",
    "for a in range(len(x)):\n",
    "\n",
    "    # L'interface étant identique, le reste du code est le même\n",
    "    tokens = tokenizer.tokenize(x[a])\n",
    "\n",
    "\n",
    "    # chargement des stopwords français\n",
    "\n",
    "    # un petit filtre\n",
    "    tokens = [token for token in tokens if token.lower() not in french_stopwords]\n",
    "\n",
    "    stopList.append(' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quand dis chers amis agit formule diplomatique expression ressens'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "\n",
    "\n",
    "countVe = CountVectorizer(strip_accents='unicode',max_df=0.95, min_df=1, analyzer=\"word\",ngram_range=(1, 3) )\n",
    "\n",
    "tfve = TfidfVectorizer(strip_accents='unicode',max_features='5000' ,max_df=0.95, min_df=1, analyzer=\"word\",ngram_range=(1, 3) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# count = countVe.fit_transform(stopList)\n",
    "idf = tfve.fit_transform(stopList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<57413x2 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 24111 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.8, class_weight={0: 2, 1: 8}, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn.naive_bayes as nb\n",
    "from sklearn import svm\n",
    "from sklearn import linear_model as lin\n",
    "\n",
    "# données ultra basiques, à remplacer par vos corpus vectorisés\n",
    "X = idf\n",
    "# X = count\n",
    "y = np.array(y)\n",
    "\n",
    "# SVM\n",
    "clf = svm.LinearSVC(C=.8,class_weight={0:2,1:8}, max_iter=1000000)\n",
    "# apprentissage\n",
    "clf.fit(X, y)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = readAFile('test.utf8')\n",
    "test=[]\n",
    "for txt in t:\n",
    "    \n",
    "    txt = re.sub(r\"<[0-9]*:[0-9]*:.>(.*)\",\"\\\\1\",txt)\n",
    "    test.append(txt)    \n",
    "\n",
    "test=[process(xi) for xi in test]\n",
    "\n",
    "testList = []\n",
    "\n",
    "# On instancie le tokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "french_stopwords = set(stopwords.words('french'))\n",
    "\n",
    "for a in range(len(test)):\n",
    "\n",
    "    # L'interface étant identique, le reste du code est le même\n",
    "    tokens = tokenizer.tokenize(test[a])\n",
    "\n",
    "\n",
    "    # chargement des stopwords français\n",
    "\n",
    "    # un petit filtre\n",
    "    tokens = [token for token in tokens if token.lower() not in french_stopwords]\n",
    "\n",
    "    testList.append(' '.join(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27162\n"
     ]
    }
   ],
   "source": [
    "print len(testList)\n",
    "a = tfve.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pp = clf.predict(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(2,len(pp)-2):\n",
    "    if pp[i-1] == pp[i+1]:\n",
    "        pp[i] = pp[i-1]\n",
    "for i in range(2,len(pp)-2):\n",
    "    if pp[i-2] == pp[i+2]:\n",
    "        pp[i] = pp[i-2]\n",
    "# for i in range(3,len(pp)-3):\n",
    "#     if pp[i-3] == pp[i+3]:\n",
    "#         pp[i] = pp[i-3]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post(y):\n",
    "    y_new=np.zeros((len(y)))\n",
    "    for i,yi in enumerate(y):\n",
    "        findex=i-5\n",
    "        lindex=i+5\n",
    "        #print 'f'+str(findex)\n",
    "        #print 'l'+str(lindex)\n",
    "        if(i-5<0):\n",
    "            findex=0\n",
    "            lindex=5+i\n",
    "        elif(i+5>len(y)):\n",
    "            findex=i-5\n",
    "            lindex=len(y)-1\n",
    "        countM=len(np.where(y[findex:lindex]==0)[0])\n",
    "        countC=len(np.where(y[findex:lindex]==1)[0])\n",
    "        y_new[i]=np.argmax([countM*2.85,countC])\n",
    "    return y_new\n",
    "\n",
    "pp = post(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "f = open('svm.txt', 'w')\n",
    "for i in pp:\n",
    "    if i == 1:\n",
    "        f.write('C\\n')\n",
    "    else:\n",
    "        f.write('M\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27162"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
