{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use french stop words instead...\n",
    "\n",
    "def filterStopWord(w):\n",
    "    pass\n",
    "\n",
    "def filter_for_tags(tagged, tags=['NN', 'JJ', 'NNP']):\n",
    "    return [item for item in tagged if item[1] in tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use punctuation instead ?\n",
    "\n",
    "def normalize(tagged):\n",
    "    return [(item.replace('.', '')) for item in tagged]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_everseen(iterable, key=None):\n",
    "    \"List unique elements, preserving order. Remember all elements ever seen.\"\n",
    "    # unique_everseen('AAAABBBCCDAABBB') --> A B C D\n",
    "    # unique_everseen('ABBCcAD', str.lower) --> A B C D\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    if key is None:\n",
    "        for element in itertools.ifilterfalse(seen.__contains__, iterable):\n",
    "            seen_add(element)\n",
    "            yield element\n",
    "    else:\n",
    "        for element in iterable:\n",
    "            k = key(element)\n",
    "            if k not in seen:\n",
    "                seen_add(k)\n",
    "                yield element\n",
    "            \n",
    "def lDistance(firstString, secondString):\n",
    "    \"Function to find the Levenshtein distance between two words/sentences - gotten from http://rosettacode.org/wiki/Levenshtein_distance#Python\"\n",
    "    if len(firstString) > len(secondString):\n",
    "        firstString, secondString = secondString, firstString\n",
    "    distances = range(len(firstString) + 1)\n",
    "    for index2, char2 in enumerate(secondString):\n",
    "        newDistances = [index2 + 1]\n",
    "        for index1, char1 in enumerate(firstString):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1])\n",
    "            else:\n",
    "                newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1])))\n",
    "        distances = newDistances\n",
    "    return distances[-1]             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildGraph(nodes):\n",
    "    \"nodes - list of hashables that represents the nodes of the graph\"\n",
    "    gr = nx.Graph() #initialize an undirected graph\n",
    "    gr.add_nodes_from(nodes)\n",
    "    nodePairs = list(itertools.combinations(nodes, 2))\n",
    "\n",
    "    #add edges to the graph (weighted by Levenshtein distance)\n",
    "    for pair in nodePairs:\n",
    "        firstString = pair[0]\n",
    "        secondString = pair[1]\n",
    "        levDistance = lDistance(firstString, secondString)\n",
    "        gr.add_edge(firstString, secondString, weight=levDistance)\n",
    "\n",
    "    return gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractKeyphrases(text):\n",
    "    #tokenize the text using nltk\n",
    "    wordTokens = nltk.word_tokenize(text)\n",
    "\n",
    "    #assign POS tags to the words in the text\n",
    "    #     tagged = nltk.pos_tag(wordTokens)\n",
    "    textlist = wordTokens\n",
    "    \n",
    "    tagged = textlist\n",
    "    tagged = normalize(tagged)\n",
    "\n",
    "    unique_word_set = unique_everseen([x for x in tagged])\n",
    "    word_set_list = list(unique_word_set)\n",
    "\n",
    "    #this will be used to determine adjacent words in order to construct keyphrases with two words\n",
    "\n",
    "    graph = buildGraph(word_set_list)\n",
    "\n",
    "    #pageRank - initial value of 1.0, error tolerance of 0,0001, \n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    #most important words in ascending order of importance\n",
    "    keyphrases = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "\n",
    "    #the number of keyphrases returned will be relative to the size of the text (a third of the number of vertices)\n",
    "    aThird = len(word_set_list) / 3\n",
    "    keyphrases = keyphrases[0:aThird+1]\n",
    "\n",
    "    #take keyphrases with multiple words into consideration as done in the paper - if two words are adjacent in the text and are selected as keywords, join them\n",
    "    #together\n",
    "    modifiedKeyphrases = set([])\n",
    "    dealtWith = set([]) #keeps track of individual keywords that have been joined to form a keyphrase\n",
    "    i = 0\n",
    "    j = 1\n",
    "    while j < len(textlist):\n",
    "        firstWord = textlist[i]\n",
    "        secondWord = textlist[j]\n",
    "        if firstWord in keyphrases and secondWord in keyphrases:\n",
    "            keyphrase = firstWord + ' ' + secondWord\n",
    "            modifiedKeyphrases.add(keyphrase)\n",
    "            dealtWith.add(firstWord)\n",
    "            dealtWith.add(secondWord)\n",
    "        else:\n",
    "            if firstWord in keyphrases and firstWord not in dealtWith: \n",
    "                modifiedKeyphrases.add(firstWord)\n",
    "\n",
    "            #if this is the last word in the text, and it is a keyword,\n",
    "            #it definitely has no chance of being a keyphrase at this point    \n",
    "            if j == len(textlist)-1 and secondWord in keyphrases and secondWord not in dealtWith:\n",
    "                modifiedKeyphrases.add(secondWord)\n",
    "        \n",
    "        i = i + 1\n",
    "        j = j + 1\n",
    "        \n",
    "    return modifiedKeyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extractSentences(text):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "    sentenceTokens = sent_detector.tokenize(text.strip())\n",
    "    \n",
    "    \n",
    "    \n",
    "    graph = buildGraph(sentenceTokens)\n",
    "\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "\n",
    "    #most important sentences in ascending order of importance\n",
    "    sentences = sorted(calculated_page_rank, key=calculated_page_rank.get, reverse=True)\n",
    "\n",
    "    #return a 3 sentence\n",
    "    summary = '\\n \\n'.join(sentences[0:3])\n",
    "    \n",
    "    return summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data.json') as data_file:    \n",
    "    data = json.load(data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Sept mythes sur le whiskey irlandais à oublier pour la Saint-Patrick...\n",
      "Reading Comment le premier coming out d’Hollywood a créé un mythe du design...\n",
      "Reading La bombe à retardement de la déscolarisation des enfants réfugiés...\n",
      "Reading Une expérience de pile ou face démonte nos fausses croyances dans la chance...\n",
      "Reading «Midnight Special», l'éveil d'un imaginaire anti-Spielberg...\n",
      "Reading En Syrie, les Occidentaux ont commis toutes les erreurs possibles...\n",
      "Reading Pour un Maghreb uni!...\n",
      "Reading Ne vous aventurez jamais à parler de pâtes Fettuccine Alfredo à un vrai Italien, JAMAIS...\n",
      "Reading Face à Zika, l’OMS est condamnée à souffler le chaud et le froid ...\n",
      "Reading Hillary Clinton a appris de ses erreurs (et déjà gagné la primaire)...\n",
      "Reading La défaite de l'homme contre la machine au go est une victoire de l'humanité...\n",
      "Reading Les tentatives désespérées des Républicains pour empêcher la nomination de Trump...\n",
      "Reading Le lourd CV des camarades de podium de Donald Trump...\n",
      "Reading CARTE. Quels sponsors pour les régions françaises?...\n",
      "Reading Et si, pour relancer l'emploi, on payait les entreprises à l'heure?...\n",
      "Reading TEST. Quel «boubour» (bourgeois-bourrin) êtes-vous?...\n",
      "Reading Il est temps d'envisager une partition de la Syrie...\n",
      "Reading Dans le grenier de Kurt Cobain...\n",
      "Reading Comment la politique allemande s'est droitisée (à l’extrême)...\n",
      "Reading L'affaire Barbarin mélange honte pour l'Église et harcèlement des médias...\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*- coding: utf-8 -*-\n",
    "import codecs\n",
    "f = codecs.open('static/processed.json', 'w', encoding='utf-8')\n",
    "for a in data:\n",
    "    print \"Reading \"+ a['titre']+\"...\"\n",
    "    text = a['content']\n",
    "    \n",
    "    a['resume'] = extractSentences(text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "l = json.dumps(data, ensure_ascii=False)\n",
    "f.write(l)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
